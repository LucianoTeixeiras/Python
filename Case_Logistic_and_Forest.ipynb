{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dfPoints = pd.read_csv(\"df_points.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>326.488285</td>\n",
       "      <td>188.988808</td>\n",
       "      <td>-312.205307</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-314.287214</td>\n",
       "      <td>307.276723</td>\n",
       "      <td>-179.037412</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-328.208910</td>\n",
       "      <td>181.627758</td>\n",
       "      <td>446.311062</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-148.658890</td>\n",
       "      <td>147.027947</td>\n",
       "      <td>-27.477959</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-467.065931</td>\n",
       "      <td>250.467651</td>\n",
       "      <td>-306.475330</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           x           y           z  label\n",
       "0           0  326.488285  188.988808 -312.205307    0.0\n",
       "1           1 -314.287214  307.276723 -179.037412    1.0\n",
       "2           2 -328.208910  181.627758  446.311062    1.0\n",
       "3           3 -148.658890  147.027947  -27.477959    1.0\n",
       "4           4 -467.065931  250.467651 -306.475330    1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some rows\n",
    "dfPoints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several warnings due to the solver argument in our models.\n",
    "# This is not a problem, so I will disable.\n",
    "# If this code is used in the future, it might be good to have warnings not disabled.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Segregate a test and training frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfPoints[['x','y','z']], dfPoints[['label']], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Logistic Regression Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression classifier on training set: 0.54\n",
      "Accuracy of Logistic regression classifier on test set: 0.53\n"
     ]
    }
   ],
   "source": [
    "# Run logistic regression (C parameter control for regularization, solver warning is not a problem)\n",
    "clf = LogisticRegression(C=0.001).fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Check accuracy\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check another evaluation metrics besides accuracy:<br>\n",
    "- Precision = TP / (TP + FP);<br>\n",
    "- Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate;<br>\n",
    "- F1 = 2 * Precision * Recall / (Precision + Recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.51\n",
      "Recall: 0.67\n",
      "F1: 0.58\n"
     ]
    }
   ],
   "source": [
    "print('Precision: {:.2f}'.format(precision_score(y_test, clf.predict(X_test))))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, clf.predict(X_test))))\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, clf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's check the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "      0.0  1.0\n",
      "0.0  503  777\n",
      "1.0  402  818\n"
     ]
    }
   ],
   "source": [
    "labels = np.unique(y_test)\n",
    "a =  confusion_matrix(y_test, clf.predict(X_test), labels=labels)\n",
    "print(\"Confusion Matrix\\n\",pd.DataFrame(a, index=labels, columns=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN =  503 \n",
      "FP =  777 \n",
      "FN =  402 \n",
      "TP =  818\n"
     ]
    }
   ],
   "source": [
    "# Another way to look at the Confusion Matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, clf.predict(X_test), labels=labels).ravel()\n",
    "print(\"TN = \", tn, \"\\nFP = \", fp, \"\\nFN = \",fn, \"\\nTP = \", tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5317302766393442\n"
     ]
    }
   ],
   "source": [
    "# Checking the AUC\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, clf.predict(X_test))\n",
    "\n",
    "x = fpr\n",
    "y = tpr \n",
    "\n",
    "# AUC\n",
    "auc = np.trapz(y,x)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the accuracy of the chosen model is not so good. However, we didn't normalize our data.<br>\n",
    "In other words, the metrics are in different scale and our model could benefit from normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression classifier on training set: 0.52\n",
      "Accuracy of Logistic regression classifier on test set: 0.51\n"
     ]
    }
   ],
   "source": [
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfPoints[['x','y','z']], dfPoints[['label']], random_state=0)\n",
    "\n",
    "# scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Run logistic regression (C parameter control for regularization won't improve in this case)\n",
    "clf = LogisticRegression().fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "# Check accuracy\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.50\n",
      "Recall: 0.72\n",
      "F1: 0.59\n",
      "\n",
      "Confusion Matrix\n",
      "      0.0  1.0\n",
      "0.0  393  887\n",
      "1.0  336  884\n",
      "\n",
      "TN =  393 \n",
      "FP =  887 \n",
      "FN =  336 \n",
      "TP =  884\n"
     ]
    }
   ],
   "source": [
    "print('Precision: {:.2f}'.format(precision_score(y_test, clf.predict(X_test_scaled))))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, clf.predict(X_test_scaled))))\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, clf.predict(X_test_scaled))))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "labels = np.unique(y_test)\n",
    "a =  confusion_matrix(y_test, clf.predict(X_test_scaled), labels=labels)\n",
    "print(\"Confusion Matrix\\n\",pd.DataFrame(a, index=labels, columns=labels))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, clf.predict(X_test_scaled), labels=labels).ravel()\n",
    "print(\"TN = \", tn, \"\\nFP = \", fp, \"\\nFN = \",fn, \"\\nTP = \", tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5158107069672131\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(y_test, clf.predict(X_test_scaled))\n",
    "\n",
    "x = fpr\n",
    "y = tpr \n",
    "\n",
    "# AUC\n",
    "auc = np.trapz(y,x)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like scale isn't the problem.<br>\n",
    "The logistic regression doesn't look like the best option here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Model Chosen by Me: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why Random Forest?</b><br>\n",
    "A few models could be tested here, such as Decision Trees, SVM and so on.<br>\n",
    "However, since I have to choose one, I would like to go with Random Forest because:<br>\n",
    "- Random Forest is a much robust model, i.e. it has a high performance when comparing to models such as logistic regression;<br>\n",
    "- Since we average several trees, there is less variance than with decision tree (which would be another good choice);<br>\n",
    "\n",
    "The main disadvantages are:<br>\n",
    "- It's less interpretable than models such as decision trees or even logistic regression;<br>\n",
    "- Depending on how much trees you are training, it can have computational costs. This is not our case, so this problem can be ignored;<br>\n",
    "- Considerable risk of overfitting (even though combining several trees reduces the risk compared to a decision tree).<br>\n",
    "\n",
    "Two nice sources to understand Random Forest:\n",
    "- https://towardsdatascience.com/why-random-forest-is-my-favorite-machine-learning-model-b97651fa3706; <br>\n",
    "- https://towardsdatascience.com/understanding-random-forest-58381e0602d2; <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RF classifier on training set: 0.98\n",
      "Accuracy of RF classifier on test set: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Carrega as bibliotecas\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfPoints[['x','y','z']], dfPoints[['label']], random_state=0)\n",
    "\n",
    "# run random forest classifier\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of RF classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of RF classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Regarding the model above:</b> the difference among the performance on the training and testing set is too large.<br>\n",
    "I.e., the model might be overfitting. By lowering the max_depth argument, we might get better results.<br>\n",
    "Let's try a new approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RF classifier on training set: 0.87\n",
      "Accuracy of RF classifier on test set: 0.76\n"
     ]
    }
   ],
   "source": [
    "# run random forest classifier\n",
    "clf = RandomForestClassifier(max_depth=15).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of RF classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of RF classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.75\n",
      "Recall: 0.75\n",
      "F1: 0.75\n",
      "\n",
      "Confusion Matrix\n",
      "      0.0  1.0\n",
      "0.0  978  302\n",
      "1.0  301  919\n",
      "\n",
      "TN =  978 \n",
      "FP =  302 \n",
      "FN =  301 \n",
      "TP =  919\n"
     ]
    }
   ],
   "source": [
    "print('Precision: {:.2f}'.format(precision_score(y_test, clf.predict(X_test))))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, clf.predict(X_test))))\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, clf.predict(X_test))))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "labels = np.unique(y_test)\n",
    "a =  confusion_matrix(y_test, clf.predict(X_test), labels=labels)\n",
    "print(\"Confusion Matrix\\n\",pd.DataFrame(a, index=labels, columns=labels))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, clf.predict(X_test), labels=labels).ravel()\n",
    "print(\"TN = \", tn, \"\\nFP = \", fp, \"\\nFN = \",fn, \"\\nTP = \", tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.758670594262295\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, clf.predict(X_test))\n",
    "\n",
    "# AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is our final choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Comparing the Results \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "As we could see, Random Forest model accuracy was way better than the logistic regression approach.<br>\n",
    "The final model needed to lower the argument max_depth to 15, so we worsen our accuracy on the training set, but improved it on the test set. <br>\n",
    "This means we lowered the risk of overfitting our model and now the model will perform better on new dataset.<br>\n",
    "Due to the advantages already mentioned above, we could improve the accuracy by 33pp.<br>\n",
    "Additionally, our model is now acceptable for use, since the logistic regression was equivalent to let things by chance.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
